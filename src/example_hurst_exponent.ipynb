{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of example_hurst_exponent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrick-kidger/Deep-Signatures/blob/master/src/example_hurst_exponent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "igKIgxFtEPQr"
      },
      "source": [
        "# Clone our repository "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pLoyWfyuKvEg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "32b15f24-e1d8-4791-c29f-75bb87fb76b8"
      },
      "source": [
        "% cd /content/\n",
        "!rm -rf Deep-Signatures/\n",
        "!git clone https://crispitagorico:Pippi2010!@github.com/patrick-kidger/Deep-Signatures.git\n",
        "% cd /content/Deep-Signatures/src\n",
        "!pip install pytorch-ignite\n",
        "!pip install fbm\n",
        "!pip install iisignature\n",
        "!pip install hurst\n",
        "!pip install nolds"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Deep-Signatures'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 1654 (delta 9), reused 13 (delta 7), pack-reused 1631\u001b[K\n",
            "Receiving objects: 100% (1654/1654), 59.25 MiB | 41.07 MiB/s, done.\n",
            "Resolving deltas: 100% (952/952), done.\n",
            "/content/Deep-Signatures/src\n",
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-ignite) (1.16.3)\n",
            "Requirement already satisfied: fbm in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fbm) (1.16.3)\n",
            "Requirement already satisfied: iisignature in /usr/local/lib/python3.6/dist-packages (0.23)\n",
            "Requirement already satisfied: numpy>1.7 in /usr/local/lib/python3.6/dist-packages (from iisignature) (1.16.3)\n",
            "Requirement already satisfied: hurst in /usr/local/lib/python3.6/dist-packages (0.0.5)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from hurst) (1.16.3)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.6/dist-packages (from hurst) (0.24.2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->hurst) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->hurst) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas>=0.18->hurst) (1.12.0)\n",
            "Requirement already satisfied: nolds in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nolds) (1.16.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from nolds) (41.0.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from nolds) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xXxSG3DCKr3P"
      },
      "source": [
        "# Deep Signatures Neural Net \n",
        "\n",
        "In this notebook we implement a Neural Network with Signatures as Activation function. \n",
        "The most straightforwrd way of incorporating signature within a Deep Learning environment is to learn a function acting on the input path via a simple NN and then taking the Signature of the output path (or the initial path augmented with the output path, in which case it simply means that we consider the identity in addition to the learned function to augment the input path). Once we compute the signature we simply add a linear layer afterwards. However, in this notebook we present various ways to repeat this procedure over and over again. We will show how one can define a Deep Signatures Network by cascading one signature layer after the other.\n",
        "\n",
        "The image of a path by the signature transform is an element of the Tensor Algebra (actually a special subspace of the tensor algebra, which is actually a Lie group). However it is no longer a path. Therefore taking the signature of the signature it doesn't make sense. Furthermore, for a path of dimension 2 and truncation level 3, the output of the signature layer will be 14-dimensional. Hence, provided we are able to map the path to another path via the signature, we ought also to be able to reduce the dimensionality of the signature layer outputs every time they are called, otherwise the output dimension will quickly explose.\n",
        "\n",
        "To reduce the dimensionality of each Signature layer output is suffices to apply a linear perceptron straight after, and define the number of output layer's nodes to be equal to the dimension we wish. This actually represent a simple projection to a lower dimensional space and allows us to control the exponential increase in output paths dimension.\n",
        "\n",
        "In order to map a path on a vector space (or Banach space) to a path on a group we propose the following two natural solutions:\n",
        "\n",
        "### 1) Lift of the input path to a path on the Lie group\n",
        "\n",
        "Let $X: [0,T] \\rightarrow R^d$ be a piecewise linear path.\n",
        "\n",
        "Let $\\mathcal{S}_m(X): \\mathcal{C}([0,T],R^d) \\rightarrow T((R^d))^{\\otimes m}$ be its truncated signature.\n",
        "\n",
        "We define the $\\textit{\"lift of X\"}$ (which is a path!) as follows:\n",
        "\n",
        "$$\\mathcal{L}(X): \\mathcal{C}([0,T],R^d) \\rightarrow \\mathcal{C}([0,T],R^d)$$\n",
        "\n",
        "$$\\{X_t\\}_{t=0}^T \\rightarrow \\{\\mathcal{S}_m(X)_{[0,t]}\\}_{t=0}^T$$\n",
        "\n",
        "where $\\mathcal{S}_m(X)_{[0,t]} := \\mathcal{S}_m(X|_{[0,t]})$.\n",
        "\n",
        "This transform enables can be seen as an operator on path space. Let's call $\\textit{lift}$ for short. We can cascade this transform many times, recursively. However the dimension of the output path after $r$ iteration will be exponentially bigger than the initial input path. In order to control this dimensionality explosion, we introduce a non-linearity $\\sigma$ (which will be learnt by the network and simply modelled as a multilinear perceptron function) between each consecutive lift-operators which acts as a projection from $R^q$ onto $R^d$, where $d$ is the dimension of the initial input path $X_t$ and $q$ id the dimension of the lift, i.e. \n",
        "\n",
        "$$\\sum_{i=0}^md^i = \\frac{d^{m+1}-1}{d-1}$$\n",
        "\n",
        "This cascade of lift transforms defines the $\\textit{signature scattering transform}$:\n",
        "\n",
        "$$\\mathcal{SS}_m^r(X) = \\mathcal{S}_m(\\sigma_1(\\mathcal{S}_m(...(\\sigma_r(\\mathcal{S}_m(X))...))$$\n",
        "\n",
        "### 2) Localised Signatures\n",
        "\n",
        "Let $X: [0,T] \\rightarrow R^d$ be a piecewise linear path. \n",
        "\n",
        "Let $\\mathcal{I} = \\{0=i_0<i_1<...<i_N=T\\}$ be its sampling index set. \n",
        "\n",
        "Let $w$ be an integer $\\in [2, N]$.\n",
        "\n",
        "We introduce the $\\textit{\"truncated local signature path of X\"}$ as follows:\n",
        "\n",
        "$$\\mathcal{S}_{m,w}^{loc} : \\mathcal{C}([0,T],R^d) \\rightarrow \\mathcal{C}([0,T],R^d)$$\n",
        "\n",
        "$$\\{X_t\\}_{t=0}^T \\rightarrow \\{\\mathcal{S}_{m,w}^{loc}(X_t)\\}_{t=w}^{N-w} := \\{\\mathcal{S}_m(X|_{[t-w,t+w]})\\}_{t=w}^{N-w}$$\n",
        "\n",
        "To reduce the dimensionality of the output pathlets we employ the same technique as before (i.e. projection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P7uYfpGxKr3U",
        "colab": {}
      },
      "source": [
        "%run base.ipynb\n",
        "%matplotlib inline\n",
        "\n",
        "# general purpose libraries\n",
        "import collections as co\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# signature utils\n",
        "from paths_transformers import *\n",
        "import iisignature\n",
        "\n",
        "# torch libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as torchdata\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils import data\n",
        "import ignite.metrics as ignite_metrics\n",
        "\n",
        "# signature pytorch layer\n",
        "from siglayer import modules, examples\n",
        "from siglayer.backend import SigLayer\n",
        "import candle\n",
        "import utils\n",
        "\n",
        "# custom modules for GRU and LSTM\n",
        "import lstm_module\n",
        "\n",
        "# package to simulate fBM and estimate Hurst\n",
        "from fbm import FBM\n",
        "from fBM_data import *\n",
        "from hurst import compute_Hc, random_walk\n",
        "from nolds import hurst_rs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SIS-M7HXegi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import importlib\n",
        "# importlib.reload(fBM_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TOGVwWQLKr3b"
      },
      "source": [
        "# Application\n",
        "\n",
        "To demonstrate the effectiveness of our method we analyse a simple application: given a realization of fractional Brownian Motion, estimate its Hurst exponent. We note that this application can be considered either a Regression problem, where the output is a scalar optimised to be as close as possible to the real exponent, or a Classification problem, where the aim to classify fBM paths into pre-established classes according to their exponent. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0qWPxHnKr3y"
      },
      "source": [
        "### Set datasets and learning parameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8JZ6vFuxKr3z",
        "colab": {}
      },
      "source": [
        "# dataset parameters\n",
        "n_paths_train=600\n",
        "n_paths_test=100 \n",
        "n_samples=300\n",
        "hurst_exponents=np.around(np.linspace(0.1,0.8,8), decimals=1).tolist()\n",
        "\n",
        "# batch and epoch sizes\n",
        "train_batch_size = 128\n",
        "val_batch_size = 128\n",
        "max_epochs = 100\n",
        "\n",
        "# target shape\n",
        "output_shape = (1,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K-52_W35Kr35"
      },
      "source": [
        "### Set optimization parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ghOjJD9gKr37",
        "colab": {}
      },
      "source": [
        "# Optimizer\n",
        "optimizer_fn = optim.Adam\n",
        "\n",
        "# Loss function\n",
        "def log_mse(x,y):\n",
        "    return torch.log(torch.mean((x-y)**2))\n",
        "loss_fn = log_mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OPDcvoUEOK7G"
      },
      "source": [
        "### Load performances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aVCms70DLeYB",
        "colab": {}
      },
      "source": [
        "# load saved performance\n",
        "history = np.load(\"figures/hurst/history.npy\", allow_pickle=True).all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KUZkgul0TrV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "acfc8c5b-54f8-4afc-dd10-08c4d0eca395"
      },
      "source": [
        "for k in history:\n",
        "  print(f'{k}: ', f'val log-loss: {history[k][\"val log-loss\"][-1]}  --- ', f'val mse: {history[k][\"val mse\"][-1]}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DeepSigNet:  val log-loss: -8.583418846130371  ---  val mse: 0.00018718400970101355\n",
            "SigNet:  val log-loss: -7.046472549438477  ---  val mse: 0.0008704739809036254\n",
            "LinearSig (no backprop, depth: 4):  val log-loss: -4.494853973388672  ---  val mse: 0.011166309118270873\n",
            "LSTM:  val log-loss: -5.540732383728027  ---  val mse: 0.003923653364181518\n",
            "GRU:  val log-loss: -5.352998733520508  ---  val mse: 0.004733933210372925\n",
            "RNN:  val log-loss: -4.7288899421691895  ---  val mse: 0.008836275339126587\n",
            "ReluNet:  val log-loss: -3.0550589561462402  ---  val mse: 0.04711994171142578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s5wgLp6r62t2"
      },
      "source": [
        "## Define dataloaders and training procedures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ejmE5-Bo--E",
        "colab_type": "text"
      },
      "source": [
        "### Relunet & RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PiyEil9a62t2",
        "colab": {}
      },
      "source": [
        "# generate dataset\n",
        "x_train, y_train, x_test, y_test = generate_data(n_paths_train, n_paths_test, n_samples, hurst_exponents)\n",
        "\n",
        "# generate torch dataloaders\n",
        "train_dataloader, test_dataloader, example_batch_x, example_batch_y = generate_torch_batched_data(x_train, \n",
        "                                                                                                  y_train, \n",
        "                                                                                                  x_test, \n",
        "                                                                                                  y_test,\n",
        "                                                                                                  train_batch_size, \n",
        "                                                                                                  val_batch_size)\n",
        "\n",
        "# define trainer\n",
        "train_model = utils.create_train_model_fn(max_epochs, optimizer_fn, loss_fn, train_dataloader, test_dataloader, example_batch_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C6nlT9o2Kr4C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "1c4df83b-2145-4dbb-f6b7-3ba129762438"
      },
      "source": [
        "relunet = examples.create_simple(output_shape, sig=False, augment_layer_sizes=(), layer_sizes = (24, 16))\n",
        "\n",
        "# load pre-trained model...\n",
        "relunet(example_batch_x)\n",
        "relunet.load_state_dict(torch.load('figures/hurst/relunet.pt'))\n",
        "relunet.eval()\n",
        "\n",
        "# ...or train from scratch and save\n",
        "# train_model(relunet, 'ReluNet', history)\n",
        "# torch.save(relunet.state_dict(), 'figures/hurst/relunet.pt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CannedNet(\n",
              "  (layers): ModuleList(\n",
              "    (0): Augment(\n",
              "      include_original=True, include_time=True\n",
              "      (convs): ModuleList()\n",
              "    )\n",
              "    (1): Flatten()\n",
              "    (2): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (3): NoInputSpec(\n",
              "      (module): Linear(in_features=602, out_features=24, bias=True)\n",
              "    )\n",
              "    (4): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (5): NoInputSpec(\n",
              "      (module): Linear(in_features=24, out_features=16, bias=True)\n",
              "    )\n",
              "    (6): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (7): NoInputSpec(\n",
              "      (module): Linear(in_features=16, out_features=1, bias=True)\n",
              "    )\n",
              "    (8): View(shape=(1,))\n",
              "    (9): Lambda(fn=<lambda>, fn_args=(), fn_kwargs={})\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jixE6MzLKr4X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "6227555b-bc21-43cc-931d-82a8ffba75ad"
      },
      "source": [
        "rnn = examples.create_windowed(output_shape, \n",
        "                               sig=False,\n",
        "                               augment_layer_sizes=(), \n",
        "                               layer_sizes_s=((64,64,32), (64,64,32)),\n",
        "                               lengths=(2,4), \n",
        "                               strides=(2,4), \n",
        "                               adjust_lengths=(0, 0),\n",
        "                               memory_sizes=(2,4),\n",
        "                               hidden_output_sizes=(4,))\n",
        "\n",
        "# load pre-trained model...\n",
        "# rnn(example_batch_x)\n",
        "rnn.load_state_dict(torch.load('figures/hurst/rnn.pt'))\n",
        "rnn.eval()\n",
        "\n",
        "# ...or train from scratch and save\n",
        "# train_model(rnn, 'RNN', history)\n",
        "# torch.save(rnn.state_dict(), 'figures/hurst/rnn.pt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-964fba77894f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# load pre-trained model...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# rnn(example_batch_x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figures/hurst/rnn.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    761\u001b[0m                     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, prefix)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, prefix)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, prefix)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, prefix)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, prefix)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, prefix)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mlocal_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             module._load_from_state_dict(\n\u001b[0;32m--> 758\u001b[0;31m                 state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n\u001b[0m\u001b[1;32m    759\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m         \u001b[0mlocal_name_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         \u001b[0mlocal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocal_name_params\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Deep-Signatures/packages/candle/modules.py\u001b[0m in \u001b[0;36m_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters_not_specified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Module has not yet been called on example inputs, so it is not yet fully specified.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Module has not yet been called on example inputs, so it is not yet fully specified."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YThvrzJBo--S",
        "colab_type": "text"
      },
      "source": [
        "### GRU & LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ts3zEWQtNW4S",
        "colab": {}
      },
      "source": [
        "# generate dataset\n",
        "x_train_lstm, y_train_lstm, x_test_lstm, y_test_lstm = generate_data(n_paths_train, \n",
        "                                                                     n_paths_test, \n",
        "                                                                     n_samples, \n",
        "                                                                     hurst_exponents, \n",
        "                                                                     flag='lstm')\n",
        "\n",
        "# generate torch dataloaders\n",
        "train_dataloader_lstm, test_dataloader_lstm, example_batch_lstm_x, example_batch_lstm_y = generate_torch_batched_data(x_train_lstm, \n",
        "                                                                                                 y_train_lstm,\n",
        "                                                                                                 x_test_lstm, \n",
        "                                                                                                 y_test_lstm,\n",
        "                                                                                                 train_batch_size,\n",
        "                                                                                                 val_batch_size)\n",
        "\n",
        "# trainer function\n",
        "train_model_lstm = utils.create_train_model_fn(max_epochs, \n",
        "                                               optimizer_fn, \n",
        "                                               loss_fn, \n",
        "                                               train_dataloader_lstm, \n",
        "                                               test_dataloader_lstm, \n",
        "                                               example_batch_lstm_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzjaWHWWp3IW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "14dc3f03-b34f-46e8-a0dc-76e4284d3e76"
      },
      "source": [
        "# LSTM\n",
        "lstmnet = lstm_module.RecurrentModel(input_dim=1, \n",
        "                                    layer_dim=4, \n",
        "                                    lstm_flag=True,\n",
        "                                    hidden_dim=64,\n",
        "                                    output_dim=1)\n",
        "\n",
        "# load pre-trained model...\n",
        "lstmnet(example_batch_lstm_x)\n",
        "lstmnet.load_state_dict(torch.load('figures/hurst/lstmnet.pt'))\n",
        "lstmnet.eval()\n",
        "\n",
        "# ...or train from scratch and save\n",
        "# train_model_lstm(lstmnet, 'LSTM', history)\n",
        "# torch.save(lstmnet.state_dict(), 'figures/hurst/lstmnet.pt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel(\n",
              "  (mod): LSTM(1, 64, num_layers=4, batch_first=True, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2bpWU-bqT5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "ab8beb3d-3a94-4800-af21-70386d90662d"
      },
      "source": [
        "# GRU\n",
        "grunet = lstm_module.RecurrentModel(input_dim=1, \n",
        "                                    layer_dim=4, \n",
        "                                    lstm_flag=False,\n",
        "                                    hidden_dim=64,\n",
        "                                    output_dim=1)\n",
        "\n",
        "# load pre-trained model...\n",
        "grunet(example_batch_lstm_x)\n",
        "grunet.load_state_dict(torch.load('figures/hurst/grunet.pt'))\n",
        "grunet.eval()\n",
        "\n",
        "# ...or train from scratch and save\n",
        "# train_model_lstm(grunet, 'GRU', history)\n",
        "# torch.save(grunet.state_dict(), 'figures/hurst/grunet.pt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel(\n",
              "  (mod): GRU(1, 64, num_layers=4, batch_first=True)\n",
              "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnQK1Eplo--l",
        "colab_type": "text"
      },
      "source": [
        "### Linear Sigs (no backprop) with time-augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RGb-mqL3Of_H",
        "colab": {}
      },
      "source": [
        "# generate dataset\n",
        "depth_lin = 4\n",
        "x_train_sig, y_train_sig, x_test_sig, y_test_sig = generate_data(n_paths_train, \n",
        "                                                                 n_paths_test, \n",
        "                                                                 n_samples, \n",
        "                                                                 hurst_exponents, \n",
        "                                                                 flag='time-transform', \n",
        "                                                                 depth_lin=depth_lin)\n",
        "\n",
        "# generate torch dataloaders\n",
        "train_dataloader_sig, test_dataloader_sig, example_batch_sig_x, example_batch_sig_y = generate_torch_batched_data(x_train_sig, \n",
        "                                                                                                                  y_train_sig,\n",
        "                                                                                                                  x_test_sig,\n",
        "                                                                                                                  y_test_sig,\n",
        "                                                                                                                  train_batch_size,\n",
        "                                                                                                                  val_batch_size)\n",
        "\n",
        "train_model_sig = utils.create_train_model_fn(max_epochs, \n",
        "                                              optimizer_fn, \n",
        "                                              loss_fn, \n",
        "                                              train_dataloader_sig, \n",
        "                                              test_dataloader_sig, \n",
        "                                              example_batch_sig_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGDg-jFp-bGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "97ed12b1-7ced-4d5a-c026-f7235f274934"
      },
      "source": [
        "sig_leastsquares = examples.create_feedforward(output_shape, sig=False, layer_sizes=(64, 64, 64, 32, 32, 32, 16, 16))\n",
        "\n",
        "# load pre-trained model...\n",
        "sig_leastsquares(example_batch_sig_x)\n",
        "sig_leastsquares.load_state_dict(torch.load('figures/hurst/sig_leastsquares.pt'))\n",
        "sig_leastsquares.eval()\n",
        "\n",
        "# ...or train from scratch and save\n",
        "# train_model_sig(sig_leastsquares, 'LinearSig (no backprop, depth: {})'.format(depth_lin), history)\n",
        "# torch.save(sig_leastsquares.state_dict(), 'figures/hurst/sig_leastsquares.pt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CannedNet(\n",
              "  (layers): ModuleList(\n",
              "    (0): Flatten()\n",
              "    (1): NoInputSpec(\n",
              "      (module): Linear(in_features=30, out_features=64, bias=True)\n",
              "    )\n",
              "    (2): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (3): NoInputSpec(\n",
              "      (module): Linear(in_features=64, out_features=64, bias=True)\n",
              "    )\n",
              "    (4): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (5): NoInputSpec(\n",
              "      (module): Linear(in_features=64, out_features=64, bias=True)\n",
              "    )\n",
              "    (6): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (7): NoInputSpec(\n",
              "      (module): Linear(in_features=64, out_features=32, bias=True)\n",
              "    )\n",
              "    (8): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (9): NoInputSpec(\n",
              "      (module): Linear(in_features=32, out_features=32, bias=True)\n",
              "    )\n",
              "    (10): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (11): NoInputSpec(\n",
              "      (module): Linear(in_features=32, out_features=32, bias=True)\n",
              "    )\n",
              "    (12): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (13): NoInputSpec(\n",
              "      (module): Linear(in_features=32, out_features=16, bias=True)\n",
              "    )\n",
              "    (14): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (15): NoInputSpec(\n",
              "      (module): Linear(in_features=16, out_features=16, bias=True)\n",
              "    )\n",
              "    (16): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (17): NoInputSpec(\n",
              "      (module): Linear(in_features=16, out_features=1, bias=True)\n",
              "    )\n",
              "    (18): View(shape=(1,))\n",
              "    (19): Lambda(fn=<lambda>, fn_args=(), fn_kwargs={})\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxvWH6SVo--r",
        "colab_type": "text"
      },
      "source": [
        "### SigNet & DeepSigNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zog1W4AVHgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "06479b7f-097e-4b11-9cba-f297e4eb7b44"
      },
      "source": [
        "signet = examples.create_simple(output_shape, \n",
        "                                   augment_kernel_size=3,\n",
        "                                   augment_layer_sizes=(4,), \n",
        "                                   sig=True, \n",
        "                                   sig_depth=2, \n",
        "                                   layer_sizes = (64, 32, 32, 32, 16, 16))\n",
        "  \n",
        "# # # load pre-trained model...\n",
        "signet(example_batch_x)\n",
        "signet.load_state_dict(torch.load('figures/hurst/signet.pt'))\n",
        "signet.eval()\n",
        "\n",
        "# ...or train from scratch and save\n",
        "# train_model(signet, 'SigNet', history)\n",
        "# torch.save(signet.state_dict(), 'figures/hurst/signet.pt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CannedNet(\n",
              "  (layers): ModuleList(\n",
              "    (0): Augment(\n",
              "      include_original=True, include_time=True\n",
              "      (convs): ModuleList(\n",
              "        (0): NoInputSpec(\n",
              "          (module): Conv1d(1, 4, kernel_size=(3,), stride=(1,))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): Signature(depth=2)\n",
              "    (2): NoInputSpec(\n",
              "      (module): Linear(in_features=42, out_features=64, bias=True)\n",
              "    )\n",
              "    (3): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (4): NoInputSpec(\n",
              "      (module): Linear(in_features=64, out_features=32, bias=True)\n",
              "    )\n",
              "    (5): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (6): NoInputSpec(\n",
              "      (module): Linear(in_features=32, out_features=32, bias=True)\n",
              "    )\n",
              "    (7): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (8): NoInputSpec(\n",
              "      (module): Linear(in_features=32, out_features=32, bias=True)\n",
              "    )\n",
              "    (9): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (10): NoInputSpec(\n",
              "      (module): Linear(in_features=32, out_features=16, bias=True)\n",
              "    )\n",
              "    (11): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (12): NoInputSpec(\n",
              "      (module): Linear(in_features=16, out_features=16, bias=True)\n",
              "    )\n",
              "    (13): Lambda(fn=relu, fn_args=(), fn_kwargs={})\n",
              "    (14): NoInputSpec(\n",
              "      (module): Linear(in_features=16, out_features=1, bias=True)\n",
              "    )\n",
              "    (15): View(shape=(1,))\n",
              "    (16): Lambda(fn=<lambda>, fn_args=(), fn_kwargs={})\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoRTOWnmV_pK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "52451b20-8683-4104-fabf-87d2b766209e"
      },
      "source": [
        "deepsignet = examples.create_windowed(output_shape, \n",
        "                                      sig=True, \n",
        "                                      sig_depth=2, \n",
        "                                      augment_layer_sizes=(16, 16, 16, 4), \n",
        "                                      augment_kernel_size=4,\n",
        "                                      lengths=(10, 10, 10), \n",
        "                                      strides=(0, 0, 0), \n",
        "                                      adjust_lengths=(5, 5, 5),\n",
        "                                      layer_sizes_s=((16, 16), (16, 16), (16, 16)), \n",
        "                                      memory_sizes=(8, 32, 8),\n",
        "                                      hidden_output_sizes=(16, 8))\n",
        "\n",
        "# load pre-trained model...\n",
        "deepsignet(example_batch_x)\n",
        "deepsignet.load_state_dict(torch.load('figures/hurst/deepsignet.pt'))\n",
        "deepsignet.eval()\n",
        "\n",
        "# # ...or train from scratch and save\n",
        "# train_model(deepsignet, 'DeepSigNet', history)\n",
        "# torch.save(deepsignet.state_dict(), 'figures/hurst/deepsignet.pt')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b2ee9ca9864a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# load pre-trained model...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdeepsignet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_batch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdeepsignet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figures/hurst/deepsignet.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdeepsignet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 777\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CannedNet:\n\tMissing key(s) in state_dict: \"layers.4.module.layers.1.module.weight\", \"layers.4.module.layers.1.module.bias\", \"layers.4.module.layers.3.module.weight\", \"layers.4.module.layers.3.module.bias\", \"layers.4.module.layers.5.module.weight\", \"layers.4.module.layers.5.module.bias\", \"layers.6.module.layers.1.module.weight\", \"layers.6.module.layers.1.module.bias\", \"layers.6.module.layers.3.module.weight\", \"layers.6.module.layers.3.module.bias\", \"layers.6.module.layers.5.module.weight\", \"layers.6.module.layers.5.module.bias\". \n\tUnexpected key(s) in state_dict: \"layers.1.module.layers.1.module.weight\", \"layers.1.module.layers.1.module.bias\", \"layers.1.module.layers.3.module.weight\", \"layers.1.module.layers.3.module.bias\", \"layers.1.module.layers.5.module.weight\", \"layers.1.module.layers.5.module.bias\", \"layers.3.module.layers.1.module.weight\", \"layers.3.module.layers.1.module.bias\", \"layers.3.module.layers.3.module.weight\", \"layers.3.module.layers.3.module.bias\", \"layers.3.module.layers.5.module.weight\", \"layers.3.module.layers.5.module.bias\". \n\tsize mismatch for layers.2.module.layers.1.module.weight: copying a param with shape torch.Size([16, 304]) from checkpoint, the shape in current model is torch.Size([16, 50]).\n\tsize mismatch for layers.2.module.layers.5.module.weight: copying a param with shape torch.Size([40, 16]) from checkpoint, the shape in current model is torch.Size([24, 16]).\n\tsize mismatch for layers.2.module.layers.5.module.bias: copying a param with shape torch.Size([40]) from checkpoint, the shape in current model is torch.Size([24])."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NHYyc-qqXasc"
      },
      "source": [
        "#  Save all loss results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "deCh0KggUeSP",
        "colab": {}
      },
      "source": [
        "# np.save(\"figures/hurst/history.npy\", history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ObD1hgF2He9c",
        "colab": {}
      },
      "source": [
        "# # list models to display\n",
        "# models = [deepsignet, signet, sig_leastsquares, lstmnet, grunet, rnn, relunet]\n",
        "\n",
        "# # store model parameters\n",
        "# params = {}\n",
        "# for k, m in zip(history, models):\n",
        "#     params[k] = utils.count_parameters(m)\n",
        "# np.save('figures/hurst/params.npy', params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o5eAKpuSKr6G"
      },
      "source": [
        "# Experiments with other methods "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuevJuxh-zQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_predictions = np.load(\"figures/hurst/all_predictions.npy\", allow_pickle=True)\n",
        "all_true = np.load(\"figures/hurst/all_true.npy\", allow_pickle=True)\n",
        "all_r2 = np.load('figures/hurst/all_r2.npy', allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2bv_-4MOff7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h_true_linsig = example_batch_sig_y.numpy()\n",
        "h_pred_linsig = sig_leastsquares(example_batch_sig_x).detach().cpu().numpy()\n",
        "\n",
        "all_predictions = all_predictions.tolist()\n",
        "all_predictions.append(h_pred_linsig)\n",
        "\n",
        "\n",
        "all_true = all_true.tolist()\n",
        "all_true.append(h_true_linsig)\n",
        "\n",
        "r2_linsig = r2_score(y_pred=h_true_linsig, y_true=h_true_linsig)\n",
        "all_r2 = all_r2.tolist()\n",
        "all_r2.append(r2_linsig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ugcwrRjaXBwl",
        "colab": {}
      },
      "source": [
        "# targets\n",
        "x_t, y_t = generate_fBM(n_paths_test, n_samples, hurst_exponents)\n",
        "h_true_sig = example_batch_y.numpy()\n",
        "h_true_rec = example_batch_lstm_y.numpy()\n",
        "h_true_linsig = example_batch_sig_y.numpy()\n",
        "\n",
        "# signature models\n",
        "h_pred_linsig = sig_leastsquares(example_batch_sig_x).detach().cpu().numpy()\n",
        "h_pred_sig = signet(example_batch_x).detach().cpu().numpy()\n",
        "h_pred_sig_deep = deepsignet(example_batch_x).detach().cpu().numpy()\n",
        "\n",
        "# feed-forward net\n",
        "h_pred_relu = relunet(example_batch_x).detach().cpu().numpy()\n",
        "\n",
        "# recurrent models\n",
        "h_pred_rnn = rnn(example_batch_x).detach().cpu().numpy()\n",
        "h_pred_lstm = lstmnet(example_batch_lstm_x).detach().cpu().numpy()\n",
        "h_pred_gru = grunet(example_batch_lstm_x).detach().cpu().numpy()\n",
        "\n",
        "# compare with other methods\n",
        "h_pred_hurst = []\n",
        "# h_pred_nolds = []\n",
        "h_pred_custom = []\n",
        "\n",
        "for x in x_t:\n",
        "  \n",
        "    # python library \"hurst\"\n",
        "#     H, _, _ = compute_Hc(x, kind='random_walk', simplified=True)\n",
        "#     h_pred_hurst.append(H)\n",
        "    \n",
        "    # custom function\n",
        "    h_pred_custom.append(hurst_fn1(x))\n",
        "\n",
        "# store predictions in list\n",
        "all_predictions = [h_pred_sig, \n",
        "                   h_pred_sig_deep, \n",
        "                   h_pred_relu,\n",
        "                   h_pred_rnn, \n",
        "                   h_pred_lstm, \n",
        "                   h_pred_gru,\n",
        "#                    h_pred_hurst\n",
        "                   h_pred_custom,\n",
        "                   h_pred_linsig]\n",
        "\n",
        "all_true = [h_true_sig,\n",
        "            h_true_sig,\n",
        "            h_true_sig,\n",
        "            h_true_sig,\n",
        "            h_true_rec,\n",
        "            h_true_rec,\n",
        "            y_t,\n",
        "#             y_t,\n",
        "            y_t,\n",
        "            h_true_linsig]\n",
        "\n",
        "# r2 scores\n",
        "r2_sig = r2_score(y_pred=h_pred_sig, y_true=h_true_sig)\n",
        "r2_sig_deep = r2_score(y_pred=h_pred_sig_deep, y_true=h_true_sig)\n",
        "r2_relu = r2_score(y_pred=h_pred_relu, y_true=h_true_sig)\n",
        "r2_rnn = r2_score(y_pred=h_pred_rnn, y_true=h_true_sig)\n",
        "r2_lstm = r2_score(y_pred=h_pred_lstm, y_true=h_true_rec)\n",
        "r2_gru = r2_score(y_pred=h_pred_gru, y_true=h_true_rec)\n",
        "# r2_hurst = r2_score(y_pred=h_pred_hurst, y_true=y_t)\n",
        "r2_custom = r2_score(y_pred=h_pred_custom, y_true=y_t)\n",
        "r2_linsig = r2_score(y_pred=h_pred_linsig, y_true=h_true_linsig)\n",
        "\n",
        "# store r2 scores in list \n",
        "all_r2 = [r2_sig, \n",
        "          r2_sig_deep, \n",
        "          r2_relu,\n",
        "          r2_rnn,\n",
        "          r2_lstm,\n",
        "          r2_gru,\n",
        "#           r2_hurst,\n",
        "          r2_custom,\n",
        "          r2_linsig]\n",
        "\n",
        "np.save(\"figures/hurst/all_predictions.npy\", all_predictions)\n",
        "np.save('figures/hurst/all_true.npy', all_true)\n",
        "np.save('figures/hurst/all_r2.npy', all_r2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qcbI5Ck4ccu_",
        "colab": {}
      },
      "source": [
        "np.save(\"figures/hurst/all_predictions.npy\", all_predictions)\n",
        "np.save('figures/hurst/all_true.npy', all_true)\n",
        "np.save('figures/hurst/all_r2.npy', all_r2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGLbVcUTP7rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}